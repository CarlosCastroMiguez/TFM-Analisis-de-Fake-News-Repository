{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open(\"Maldita_Dataset_27-06-2020\",'rb')\n",
    "dataset = pickle.load(infile)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para instalar el NLTK se me abre una interfaz de usuario en otra ventana.\n",
    "\n",
    "#IMPORTS FROM BLOG\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer  #--------------\n",
    "\n",
    "#IMPORTS FROM NESTOR.\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SpanishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_keywords(words):\n",
    "    \"\"\"\n",
    "    Return the top10 frequent words.\n",
    "    :param list word: List containing all the words\n",
    "    :return: ordered list with the top-10 most frequent words (in descending order)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    counts = Counter(words)\n",
    "    ordered_list = sorted(words, key=counts.get, reverse=True)\n",
    "    unique_list = []\n",
    "    \n",
    "    #print(counts.most_common()[0:10])\n",
    "    #print(type(counts.most_common()[0:10]))\n",
    "    \n",
    "    #10 most common\n",
    "    most_com = counts.most_common()[0:10]\n",
    "    \n",
    "    return most_com\n",
    "    \n",
    "    #Ahora me devuelve una lista de tuplas con el numero de repeticiones de cada valor\n",
    "    #for ol in ordered_list:\n",
    "    #    if ol not in unique_list:\n",
    "    #        unique_list.append(ol)\n",
    "    #return unique_list[:10]\n",
    "\n",
    "def show_top_10_keywords(processed_articles):\n",
    "    # Show the top-10 most frequent words\n",
    "    for article_words in processed_articles:\n",
    "        # Get te top-10 words\n",
    "        top_10_words = get_top_10_keywords(article_words)\n",
    "        \n",
    "        print(\"\\nTOP 10 words:\")\n",
    "        for el in top_10_words:\n",
    "            print(el[0], ':', el[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circula por WhatsApp una cadena en la que se dice que Correos estaÃÅ \"redondeando\" los precios de sus servicios y que ese dinero se destina para \"pagar mascarillas y elementos de proteccioÃÅn para los funcionarios de Correos\". Es un bulo: se trata de una campanÃÉa que han lanzado con el objetivo de recaudar fondos para la investigacioÃÅn contra la COVID-19.Desde Correos nos explican que han \"incorporado en la red de oficinas la figura del 'redondeo solidario' en su pasarela de pago, para ayudar a recaudar fondos y hacer frente al COVID‚Äê19\". Para ello, \"los clientes de Correos pueden redondear el importe final de su compra y donar los ceÃÅntimos restantes a la lucha contra el coronavirus cada vez que utilicen su tarjeta como medio de pago\".SeguÃÅn Correos, todo lo recaudado se destinaraÃÅ a \"un proyecto conjunto de IrsiCaixa con el Barcelona Supercomputing Center y el Centro de InvestigacioÃÅn en Sanidad Animal del Instituto de InvestigacioÃÅn y TecnologiÃÅa Agroalimentarias\" que tiene como objetivo \"desarrollar vacunas que inmunicen frente a la enfermedad, anticuerpos capaces de neutralizar el virus y faÃÅrmacos para combatir la pandemia del COVID-19\". Es decir, que el dinero no es para comprar mascarillas para los empleados de Correos.En su paÃÅgina web, Correos explica que, a fecha de 19 de mayo, se han recaudado 25.400‚Ç¨ de los 30.000‚Ç¨ que necesitan para llevar a cabo las investigaciones contra la COVID-19, las cuales detallan en esa misma paÃÅgina:Como se puede comprobar, en ninguÃÅn lugar indican que lo recaudado vaya a destinarse a la compra de material de proteccioÃÅn para los empleados de Correos. Tampoco se hace referencia a ello en el comunicado que publicoÃÅ IrsiCaixa para anunciar el proyecto, ni en el tuit en el que agradeciÃÅan la colaboracioÃÅn de los clientes de Correos.Muchas gracias a los clientes de @Correos, que con su generosidad nos permiten seguir avanzando en la investigacioÃÅn contra el SARS-CoV-2 üí™ https://t.co/WRtW6esaOnPor otra parte, el pasado 7 de mayo se publicoÃÅ en la Plataforma de ContratacioÃÅn del Sector PuÃÅblico la adjudicacioÃÅn del contrato de compra de 4.750.000 mascarillas para Correos, por un valor de 11.815.625‚Ç¨. Puedes consultarlo aquiÃÅ.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0:5][1][\"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#De los diccionarios de articulos ** me quedo solo con el contenido de x articulos del dataset y los introduzco en una lista\n",
    "#de contenidos.\n",
    "# **article = {\"Title\": title, \"Publication_date\": date, \"Content\": content, \"URL\": url, \"Fuente\": \"Maldita.es\"}\n",
    "\n",
    "content_list = []\n",
    "for a in dataset[0:2]:\n",
    "    content_list.append(a['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 1...\n",
      "\n",
      "TOP 10 words:\n",
      "de : 33\n",
      "en : 27\n",
      ", : 24\n",
      "el : 20\n",
      ". : 20\n",
      "la : 15\n",
      "un : 15\n",
      "que : 14\n",
      "por : 12\n",
      "se : 9\n",
      "\n",
      "TOP 10 words:\n",
      "de : 29\n",
      "que : 15\n",
      "la : 14\n",
      ", : 14\n",
      "el : 13\n",
      "en : 11\n",
      "para : 11\n",
      "a : 10\n",
      "Correos : 9\n",
      "los : 9\n"
     ]
    }
   ],
   "source": [
    "def exercise_1(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 1:\n",
    "        * Extract the top-10 most frequent words appearing in the articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 1...\")\n",
    "    # Show the top-10 most frequent words\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        # Get the list of words in the article\n",
    "        article_words = word_tokenize(article)\n",
    "        processed_articles.append(article_words)\n",
    "    return processed_articles\n",
    "\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_1(content_list)\n",
    "#print(processed_articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 2...\n",
      "\n",
      "TOP 10 words:\n",
      ", : 24\n",
      ". : 20\n",
      "( : 8\n",
      ") : 8\n",
      "viÃÅdeo : 7\n",
      "puerto : 5\n",
      "bulo : 5\n",
      "si : 5\n",
      "alarma : 4\n",
      "coronavirus : 4\n",
      "\n",
      "TOP 10 words:\n",
      ", : 14\n",
      "Correos : 9\n",
      "`` : 6\n",
      "'' : 6\n",
      ". : 6\n",
      "mascarillas : 3\n",
      ": : 3\n",
      "clientes : 3\n",
      "compra : 3\n",
      "recaudado : 3\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_all_news(news, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Method to remove the stop words from a sentence.\n",
    "    :param str sentence: List of all news\n",
    "    :param list stopwords: List containing the stopwords to remove from the sentences\n",
    "    :return: list of news with the stopwords removed\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    important_words_in_news=[]\n",
    "    for one_news in news:\n",
    "        new_words = []\n",
    "        # Split the sentence into separate words\n",
    "        words = word_tokenize(one_news)\n",
    "        #print(words)\n",
    "        \n",
    "        for word in words:\n",
    "        # Only keep the important words\n",
    "            if word not in custom_stopwords:\n",
    "                new_words.append(word)         \n",
    "        important_words_in_news.append(new_words)\n",
    "        \n",
    "    return important_words_in_news\n",
    "\n",
    "\n",
    "\n",
    "def exercise_2(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 2: Most of the results are stopwords. Let's remove them from the articles first\n",
    "        * Perform some normalization:\n",
    "            * Remove the stopwords (we can also use custom stop words)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 2...\")\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_2(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "* Convierte el texto a min√∫scula\n",
    "* Quita las stopwords\n",
    "* Quita signos de puntuaci√≥n\n",
    "* Quita los acentos\n",
    "* Extrae las 10 palabras m√°s frecuentes de los art√≠culos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 3...\n",
      "articles len 3029\n",
      "articles len 2196\n",
      "lower len 3029\n",
      "lower len 2196\n",
      "accent_articles len 2979\n",
      "accent_articles len 2167\n",
      "accent_articles2 len 2901\n",
      "accent_articles2 len 2110\n",
      "articles_no_stopwords len 253\n",
      "articles_no_stopwords len 173\n",
      "\n",
      "TOP 10 words:\n",
      "video : 7\n",
      "puerto : 5\n",
      "bulo : 5\n",
      "si : 5\n",
      "alarma : 4\n",
      "coronavirus : 4\n",
      "contamos : 4\n",
      "redes : 4\n",
      "ciudad : 4\n",
      "grabado : 4\n",
      "\n",
      "TOP 10 words:\n",
      "correos : 9\n",
      "investigacion : 4\n",
      "mascarillas : 3\n",
      "covid19 : 3\n",
      "clientes : 3\n",
      "compra : 3\n",
      "recaudado : 3\n",
      "publico : 3\n",
      "dinero : 2\n",
      "proteccion : 2\n"
     ]
    }
   ],
   "source": [
    "def exercise_3(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 3: There is further preprocessing we can perform\n",
    "        * Perform some normalization:\n",
    "            * Lowercase all words\n",
    "            * Remove all the stopwords\n",
    "            * Remove punctuation signs\n",
    "            * Remove all accents from the words\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 3...\")\n",
    "    \n",
    "    #print(articles)\n",
    "    print('articles len', len(articles[0]))\n",
    "    print('articles len', len(articles[1]))\n",
    "    \n",
    "    # Lowercase all the articles\n",
    "    lower_articles = to_lowercase(articles)\n",
    "    \n",
    "    #print(lower_articles)\n",
    "    print('lower len', len(lower_articles[0]))\n",
    "    print('lower len', len(lower_articles[1]))\n",
    "    \n",
    "#     Create a translation table\n",
    "#     table = dict(zip( #  Quitar tildes\n",
    "#         [ord(x) for x in u'√°√©√≠√≥√∫√ºiÃÅ'],\n",
    "#         [ord(x) for x in u'aeiouui']\n",
    "#     ))\n",
    "#     \n",
    "#     print(type(string))\n",
    "#     accent_string = [article.translate(table) for article in lower_articles]\n",
    "#     print(accent_string)\n",
    "    \n",
    "    accent_articles = remove_non_ascii(lower_articles)\n",
    "    print('accent_articles len', len(accent_articles[0]))\n",
    "    print('accent_articles len', len(accent_articles[1]))\n",
    "    \n",
    "    #Remove the `` and '' and probably more useless stuff\n",
    "    accent_articles2 = remove_punctuation(accent_articles)\n",
    "    print('accent_articles2 len', len(accent_articles2[0]))\n",
    "    print('accent_articles2 len', len(accent_articles2[1]))\n",
    "    \n",
    "    # Prepare our custom list of stopwords\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    \n",
    "    # Add all the punctuation signs to the list of stopwords\n",
    "    custom_stopwords += list(string.punctuation)\n",
    "    \n",
    "    # lowercase all the stopwords\n",
    "    my_stopwords = to_lowercase(custom_stopwords)\n",
    "    \n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(accent_articles2, my_stopwords)\n",
    "    print('articles_no_stopwords len', len(articles_no_stopwords[0]))\n",
    "    print('articles_no_stopwords len', len(articles_no_stopwords[1]))\n",
    "    \n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_3(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
