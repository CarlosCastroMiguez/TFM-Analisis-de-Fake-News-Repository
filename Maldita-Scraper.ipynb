{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://maldita.es/malditobulo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagina status code 200\n"
     ]
    }
   ],
   "source": [
    "#print (\"pagina\", page)\n",
    "print (\"pagina status code\", page.status_code)\n",
    "#print (\"contenido de la pagina\", page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_urls(link, lista_enlaces):\n",
    "    \n",
    "    page = requests.get(link)\n",
    "    \n",
    "    soup2 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    todo2 = soup2.find('div', class_='post-cards-list')\n",
    "    urls = todo2.find_all('a')\n",
    "\n",
    "    enlaces = []\n",
    "    for url in urls:\n",
    "        #print(url.attrs['href'])\n",
    "        enlaces.append(url.attrs['href'])\n",
    "    for e in enlaces[1::2]:\n",
    "        lista_enlaces.append(e)\n",
    "    lista_enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scraping_article(url):\n",
    "    \n",
    "    #article = {\"Title\":\"Titulo_noticia\", \"Publication_date\":\"Fecha_noticia\", \"Content\":\"Article_content\"}\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup3 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    title = soup3.find(\"h1\",class_=\"content-title pl-3 pr-3 mb-3 mt-0\").get_text()\n",
    "    #print(title)\n",
    "\n",
    "    date = soup3.find(\"h6\",class_=\"pl-3 pr-3 mt-3 text-uppercase\").get_text().strip('\\n')\n",
    "    #print(date)\n",
    "    \n",
    "    content_list = []\n",
    "    body = soup3.find(\"div\", class_=\"article-body\")\n",
    "    body2 = body.find_all(\"p\", class_=\"\")\n",
    "\n",
    "    for e in body2:\n",
    "        content_list.append(e.get_text().replace(\"\\xa0\", \" \"))\n",
    "    \n",
    "    content = ''.join(content_list)\n",
    "    \n",
    "    article = {\"Title\": title, \"Publication_date\": date, \"Content\": content, \"URL\": url, \"Fuente\": \"Maldita.es\"}\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando URLs...\n"
     ]
    }
   ],
   "source": [
    "lista_enlaces = []\n",
    "suficiente = True\n",
    "n_pag = 1\n",
    "n_noticias = 1#Las noticias van en bloques de 24. La carga computacional aumentaria mucho si quisiera que fuera mas preciso\n",
    "                #Pero para este caso de uso y estudio no es necesario.\n",
    "\n",
    "#Cargo las URLS.\n",
    "print('Cargando URLs...')\n",
    "\n",
    "while (len(lista_enlaces) <= n_noticias-1 and suficiente):\n",
    "    link =  \"https://maldita.es/malditobulo/page/\" + str(n_pag) + \"/\"\n",
    "    arrayurls= get_urls(link, lista_enlaces)\n",
    "    n_pag = n_pag + 1\n",
    "\n",
    "    if (n_pag >=10):\n",
    "        suficiente=False\n",
    "        \n",
    "lista_articulos = []   \n",
    "\n",
    "for e in lista_enlaces:\n",
    "    articulo = scraping_article(e)\n",
    "    lista_articulos.append(articulo)\n",
    "    \n",
    "print(len(lista_enlaces))\n",
    "print(lista_enlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_articulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "date = date.today().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "\n",
    "filename = 'Maldita_FN_' + date\n",
    "print(filename)\n",
    "\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(lista_articulos, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(filename,'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "print(new_dict==lista_articulos)\n",
    "print(type(new_dict))\n",
    "new_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
