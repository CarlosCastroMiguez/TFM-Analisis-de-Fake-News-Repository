{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open(\"Maldita_Dataset_02-07-2020\",'rb')\n",
    "dataset = pickle.load(infile)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para instalar el NLTK se me abre una interfaz de usuario en otra ventana.\n",
    "#IMPORTS FROM BLOG\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#IMPORTS FROM NESTOR.\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_keywords(words):\n",
    "    \"\"\"\n",
    "    Return the top10 frequent words.\n",
    "    :param list word: List containing all the words\n",
    "    :return: ordered list with the top-10 most frequent words (in descending order)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    counts = Counter(words)\n",
    "    ordered_list = sorted(words, key=counts.get, reverse=True)\n",
    "    unique_list = []\n",
    "    \n",
    "    #print(counts.most_common()[0:10])\n",
    "    #print(type(counts.most_common()[0:10]))\n",
    "    \n",
    "    #10 most common\n",
    "    most_com = counts.most_common()[0:10]\n",
    "    \n",
    "    return most_com\n",
    "    \n",
    "    #Ahora me devuelve una lista de tuplas con el numero de repeticiones de cada valor\n",
    "    #for ol in ordered_list:\n",
    "    #    if ol not in unique_list:\n",
    "    #        unique_list.append(ol)\n",
    "    #return unique_list[:10]\n",
    "\n",
    "def show_top_10_keywords(processed_articles):\n",
    "    # Show the top-10 most frequent words\n",
    "    for article_words in processed_articles:\n",
    "        # Get te top-10 words\n",
    "        top_10_words = get_top_10_keywords(article_words)\n",
    "        \n",
    "        print(\"\\nTOP 10 words:\")\n",
    "        for el in top_10_words:\n",
    "            print(el[0], ':', el[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#De los diccionarios de articulos ** me quedo solo con el contenido de x articulos del dataset y los introduzco en una lista\n",
    "#de contenidos.\n",
    "# **article = {\"Title\": title, \"Publication_date\": date, \"Content\": content, \"URL\": url, \"Fuente\": \"Maldita.es\"}\n",
    "content_list = []\n",
    "for a in dataset[0:2]:\n",
    "    content_list.append(a['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 1...\n",
      "\n",
      "TOP 10 words:\n",
      "de : 33\n",
      "en : 27\n",
      ", : 24\n",
      "el : 20\n",
      ". : 20\n",
      "la : 15\n",
      "un : 15\n",
      "que : 14\n",
      "por : 12\n",
      "se : 9\n",
      "\n",
      "TOP 10 words:\n",
      "de : 29\n",
      "que : 15\n",
      "la : 14\n",
      ", : 14\n",
      "el : 13\n",
      "en : 11\n",
      "para : 11\n",
      "a : 10\n",
      "Correos : 9\n",
      "los : 9\n"
     ]
    }
   ],
   "source": [
    "def exercise_1(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 1:\n",
    "        * Extract the top-10 most frequent words appearing in the articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 1...\")\n",
    "    # Show the top-10 most frequent words\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        # Get the list of words in the article\n",
    "        article_words = word_tokenize(article)\n",
    "        processed_articles.append(article_words)\n",
    "    return processed_articles\n",
    "\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_1(content_list)\n",
    "#print(processed_articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 2...\n",
      "\n",
      "TOP 10 words:\n",
      ", : 24\n",
      ". : 20\n",
      "( : 8\n",
      ") : 8\n",
      "vídeo : 7\n",
      "puerto : 5\n",
      "bulo : 5\n",
      "si : 5\n",
      "alarma : 4\n",
      "coronavirus : 4\n",
      "\n",
      "TOP 10 words:\n",
      ", : 14\n",
      "Correos : 9\n",
      "`` : 6\n",
      "'' : 6\n",
      ". : 6\n",
      "mascarillas : 3\n",
      ": : 3\n",
      "clientes : 3\n",
      "compra : 3\n",
      "recaudado : 3\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_all_news(news, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Method to remove the stop words from a sentence.\n",
    "    :param str sentence: List of all news\n",
    "    :param list stopwords: List containing the stopwords to remove from the sentences\n",
    "    :return: list of news with the stopwords removed\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    important_words_in_news=[]\n",
    "    for one_news in news:\n",
    "        new_words = []\n",
    "        # Split the sentence into separate words\n",
    "        words = word_tokenize(one_news)\n",
    "        #print(words)\n",
    "        \n",
    "        for word in words:\n",
    "        # Only keep the important words\n",
    "            if word not in custom_stopwords:\n",
    "                new_words.append(word)         \n",
    "        important_words_in_news.append(new_words)\n",
    "        \n",
    "    return important_words_in_news\n",
    "\n",
    "def exercise_2(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 2: Most of the results are stopwords. Let's remove them from the articles first\n",
    "        * Perform some normalization:\n",
    "            * Remove the stopwords (we can also use custom stop words)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 2...\")\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_2(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* Convierte el texto a minúscula\n",
    "* Quita las stopwords\n",
    "* Quita signos de puntuación\n",
    "* Quita los acentos\n",
    "* Extrae las 10 palabras más frecuentes de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING...\n",
      "articles len 3029\n",
      "articles len 2196\n",
      "lower len 3029\n",
      "lower len 2196\n",
      "accent_articles len 2979\n",
      "accent_articles len 2167\n",
      "accent_articles2 len 2901\n",
      "accent_articles2 len 2110\n",
      "articles_no_stopwords len 253\n",
      "articles_no_stopwords len 173\n",
      "\n",
      "TOP 10 words:\n",
      "video : 8\n",
      "puerto : 5\n",
      "bulo : 5\n",
      "si : 5\n",
      "alarma : 4\n",
      "coronavirus : 4\n",
      "contamos : 4\n",
      "redes : 4\n",
      "ciudad : 4\n",
      "grabado : 4\n",
      "\n",
      "TOP 10 words:\n",
      "correos : 9\n",
      "investigacion : 4\n",
      "mascarillas : 3\n",
      "covid19 : 3\n",
      "clientes : 3\n",
      "compra : 3\n",
      "recaudado : 3\n",
      "publico : 3\n",
      "dinero : 2\n",
      "proteccion : 2\n"
     ]
    }
   ],
   "source": [
    "def exercise_3(articles):\n",
    "    \"\"\"\n",
    "    Preprocessing: There is further preprocessing we can perform\n",
    "        * Perform some normalization:\n",
    "            * Lowercase all words\n",
    "            * Remove all the stopwords\n",
    "            * Remove punctuation signs\n",
    "            * Remove all accents from the words\n",
    "            * Stem all the words (Stemming = obtaining something similar to the \"root\" form of the word)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"PREPROCESSING...\")\n",
    "    \n",
    "    #print(articles)\n",
    "    print('articles len', len(articles[0]))\n",
    "    print('articles len', len(articles[1]))\n",
    "    \n",
    "    # Lowercase all the articles\n",
    "    lower_articles = to_lowercase(articles)\n",
    "    \n",
    "    #print(lower_articles)\n",
    "    print('lower len', len(lower_articles[0]))\n",
    "    print('lower len', len(lower_articles[1]))\n",
    "    \n",
    "#     Create a translation table\n",
    "#     table = dict(zip( #  Quitar tildes\n",
    "#         [ord(x) for x in u'áéíóúüí'],\n",
    "#         [ord(x) for x in u'aeiouui']\n",
    "#     ))\n",
    "#     \n",
    "#     print(type(string))\n",
    "#     accent_string = [article.translate(table) for article in lower_articles]\n",
    "#     print(accent_string)\n",
    "    \n",
    "    accent_articles = remove_non_ascii(lower_articles)\n",
    "    print('accent_articles len', len(accent_articles[0]))\n",
    "    print('accent_articles len', len(accent_articles[1]))\n",
    "    \n",
    "    #Remove the `` and '' and probably more useless stuff\n",
    "    accent_articles2 = remove_punctuation(accent_articles)\n",
    "    print('accent_articles2 len', len(accent_articles2[0]))\n",
    "    print('accent_articles2 len', len(accent_articles2[1]))\n",
    "    \n",
    "    # Prepare our custom list of stopwords\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    \n",
    "    # Add all the punctuation signs to the list of stopwords\n",
    "    custom_stopwords += list(string.punctuation)\n",
    "    \n",
    "    # lowercase all the stopwords\n",
    "    my_stopwords = to_lowercase(custom_stopwords)\n",
    "    \n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(accent_articles2, my_stopwords)\n",
    "    print('articles_no_stopwords len', len(articles_no_stopwords[0]))\n",
    "    print('articles_no_stopwords len', len(articles_no_stopwords[1]))\n",
    "    \n",
    "    # Get the spanish stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_articles = []\n",
    "    for article in articles_no_stopwords:\n",
    "        article_words = []\n",
    "        # The article contains all the words. Separate them\n",
    "        for word in article:\n",
    "            article_words.append(lemmatizer.lemmatize(word))\n",
    "        # The next step expects a string. Let's concatenate the separate words using blanks.\n",
    "        lemmatized_articles.append(article_words)\n",
    "        \n",
    "    return lemmatized_articles\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_3(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['alarma',\n",
       "  'coronavirus',\n",
       "  'circulado',\n",
       "  'fotos',\n",
       "  'video',\n",
       "  'animales',\n",
       "  'campando',\n",
       "  'anchas',\n",
       "  'varias',\n",
       "  'ciudades',\n",
       "  'ciervos',\n",
       "  'delfines',\n",
       "  'flamenco',\n",
       "  'supuestamente',\n",
       "  'ocupado',\n",
       "  'espacio',\n",
       "  'humanos',\n",
       "  'dejado',\n",
       "  'confinamiento',\n",
       "  'embargo',\n",
       "  'aunque',\n",
       "  'imagenes',\n",
       "  'tipo',\n",
       "  'podido',\n",
       "  'ver',\n",
       "  'semanas',\n",
       "  'habeis',\n",
       "  'preguntado',\n",
       "  'tomado',\n",
       "  'crisis',\n",
       "  'coronavirus',\n",
       "  'montaje',\n",
       "  'contamos',\n",
       "  'ejemplosesta',\n",
       "  'circulando',\n",
       "  'video',\n",
       "  'redes',\n",
       "  'sociales',\n",
       "  'aparecen',\n",
       "  'delfines',\n",
       "  'nadando',\n",
       "  'puerto',\n",
       "  'deportivo',\n",
       "  'supuestamente',\n",
       "  'debido',\n",
       "  'inactividad',\n",
       "  'puerto',\n",
       "  'provocada',\n",
       "  'crisis',\n",
       "  'sanitarium',\n",
       "  'coronavirus',\n",
       "  'video',\n",
       "  'movido',\n",
       "  'diciendo',\n",
       "  'trata',\n",
       "  'paseo',\n",
       "  'maritimo',\n",
       "  'palma',\n",
       "  'mallorca',\n",
       "  'puerto',\n",
       "  'denia',\n",
       "  'alicante',\n",
       "  'moraira',\n",
       "  'alicante',\n",
       "  'premia',\n",
       "  'mar',\n",
       "  'barcelona',\n",
       "  'bulo',\n",
       "  'imagenes',\n",
       "  'grabadas',\n",
       "  'puerto',\n",
       "  'deportivo',\n",
       "  'atakoy',\n",
       "  'ciudad',\n",
       "  'estambul',\n",
       "  'turquia',\n",
       "  'contamos',\n",
       "  'aquinos',\n",
       "  'estais',\n",
       "  'preguntando',\n",
       "  'si',\n",
       "  'video',\n",
       "  'ven',\n",
       "  'ciervos',\n",
       "  'paseando',\n",
       "  'calle',\n",
       "  'circula',\n",
       "  'afirmando',\n",
       "  'municipio',\n",
       "  'ruidera',\n",
       "  'provincia',\n",
       "  'ciudad',\n",
       "  'real',\n",
       "  'realmente',\n",
       "  'grabado',\n",
       "  'ahi',\n",
       "  'alarma',\n",
       "  'tambien',\n",
       "  'movido',\n",
       "  'si',\n",
       "  'teruel',\n",
       "  'pobla',\n",
       "  'lillet',\n",
       "  'barcelona',\n",
       "  'ezcaray',\n",
       "  'pallars',\n",
       "  'bulo',\n",
       "  'video',\n",
       "  'actual',\n",
       "  'grabado',\n",
       "  'espana',\n",
       "  'sino',\n",
       "  'italia',\n",
       "  'puedes',\n",
       "  'leer',\n",
       "  'ma',\n",
       "  'aquinos',\n",
       "  'habeis',\n",
       "  'preguntado',\n",
       "  'tambien',\n",
       "  'video',\n",
       "  'ciervo',\n",
       "  'dando',\n",
       "  'brincos',\n",
       "  'playa',\n",
       "  'mueve',\n",
       "  'si',\n",
       "  'sido',\n",
       "  'grabado',\n",
       "  'playa',\n",
       "  'matalascanas',\n",
       "  'huelva',\n",
       "  'alarma',\n",
       "  'decretado',\n",
       "  'espana',\n",
       "  'covid19',\n",
       "  'embargo',\n",
       "  'video',\n",
       "  'actual',\n",
       "  'puedes',\n",
       "  'leer',\n",
       "  'ma',\n",
       "  'articuloesta',\n",
       "  'circulando',\n",
       "  'redes',\n",
       "  'sociales',\n",
       "  'publicacion',\n",
       "  'acompanada',\n",
       "  'imagen',\n",
       "  'gran',\n",
       "  'canal',\n",
       "  'ciudad',\n",
       "  'italiana',\n",
       "  'venecia',\n",
       "  'repleta',\n",
       "  'califica',\n",
       "  'cisnes',\n",
       "  'rosados',\n",
       "  'aunque',\n",
       "  'parecen',\n",
       "  'flamenco',\n",
       "  'si',\n",
       "  'real',\n",
       "  'contenido',\n",
       "  'asegura',\n",
       "  'refleja',\n",
       "  'llegada',\n",
       "  'cisnes',\n",
       "  'rosados',\n",
       "  'hacian',\n",
       "  'hace',\n",
       "  'ano',\n",
       "  'efectos',\n",
       "  'pandemia',\n",
       "  'embarcaciones',\n",
       "  'turistas',\n",
       "  'canal',\n",
       "  'embargo',\n",
       "  'bulo',\n",
       "  'trata',\n",
       "  'montaje',\n",
       "  'contamos',\n",
       "  'ma',\n",
       "  'aquise',\n",
       "  'viralizado',\n",
       "  'redes',\n",
       "  'video',\n",
       "  'ballena',\n",
       "  'nadando',\n",
       "  'puerto',\n",
       "  'supuestamente',\n",
       "  'habria',\n",
       "  'sido',\n",
       "  'grabado',\n",
       "  'rio',\n",
       "  'guadalquivir',\n",
       "  'paso',\n",
       "  'sevilla',\n",
       "  'relacionandolo',\n",
       "  'alarma',\n",
       "  'decretado',\n",
       "  'espana',\n",
       "  'covid19',\n",
       "  'bulo',\n",
       "  'imagenes',\n",
       "  'tomadas',\n",
       "  '2017',\n",
       "  'ventura',\n",
       "  'harbor',\n",
       "  'marina',\n",
       "  'california',\n",
       "  'contamos',\n",
       "  'ma',\n",
       "  'articuloesta',\n",
       "  'circulando',\n",
       "  'redes',\n",
       "  'sociales',\n",
       "  'imagen',\n",
       "  'leon',\n",
       "  'medio',\n",
       "  'calle',\n",
       "  'noche',\n",
       "  'si',\n",
       "  'ocurriendo',\n",
       "  'rusia',\n",
       "  'actual',\n",
       "  'brote',\n",
       "  'coronavirus',\n",
       "  'mensaje',\n",
       "  'viraliza',\n",
       "  'tambien',\n",
       "  'ingles',\n",
       "  'dice',\n",
       "  'gobierno',\n",
       "  'ruso',\n",
       "  'libera',\n",
       "  '700',\n",
       "  'leone',\n",
       "  'mantener',\n",
       "  'gente',\n",
       "  'casas',\n",
       "  'bulo',\n",
       "  'imagen',\n",
       "  'real',\n",
       "  'tomada',\n",
       "  'ciudad',\n",
       "  'sudafricana',\n",
       "  'johannesburgo',\n",
       "  '2016',\n",
       "  'puedes',\n",
       "  'leerlo',\n",
       "  'completo',\n",
       "  'aqui'],\n",
       " ['circula',\n",
       "  'whatsapp',\n",
       "  'cadena',\n",
       "  'dice',\n",
       "  'correos',\n",
       "  'redondeando',\n",
       "  'precios',\n",
       "  'servicios',\n",
       "  'dinero',\n",
       "  'destina',\n",
       "  'pagar',\n",
       "  'mascarillas',\n",
       "  'elementos',\n",
       "  'proteccion',\n",
       "  'funcionarios',\n",
       "  'correos',\n",
       "  'bulo',\n",
       "  'trata',\n",
       "  'campana',\n",
       "  'lanzado',\n",
       "  'objetivo',\n",
       "  'recaudar',\n",
       "  'fondos',\n",
       "  'investigacion',\n",
       "  'covid19desde',\n",
       "  'correos',\n",
       "  'explican',\n",
       "  'incorporado',\n",
       "  'red',\n",
       "  'oficinas',\n",
       "  'figura',\n",
       "  'redondeo',\n",
       "  'solidario',\n",
       "  'pasarela',\n",
       "  'pago',\n",
       "  'ayudar',\n",
       "  'recaudar',\n",
       "  'fondos',\n",
       "  'hacer',\n",
       "  'frente',\n",
       "  'covid19',\n",
       "  'ello',\n",
       "  'clientes',\n",
       "  'correos',\n",
       "  'pueden',\n",
       "  'redondear',\n",
       "  'importe',\n",
       "  'final',\n",
       "  'compra',\n",
       "  'donar',\n",
       "  'centimo',\n",
       "  'restantes',\n",
       "  'lucha',\n",
       "  'coronavirus',\n",
       "  'cada',\n",
       "  'vez',\n",
       "  'utilicen',\n",
       "  'tarjeta',\n",
       "  'medio',\n",
       "  'pagosegun',\n",
       "  'correos',\n",
       "  'recaudado',\n",
       "  'destinara',\n",
       "  'proyecto',\n",
       "  'conjunto',\n",
       "  'irsicaixa',\n",
       "  'barcelona',\n",
       "  'supercomputing',\n",
       "  'center',\n",
       "  'centro',\n",
       "  'investigacion',\n",
       "  'sanidad',\n",
       "  'animal',\n",
       "  'instituto',\n",
       "  'investigacion',\n",
       "  'tecnologia',\n",
       "  'agroalimentarias',\n",
       "  'objetivo',\n",
       "  'desarrollar',\n",
       "  'vacunas',\n",
       "  'inmunicen',\n",
       "  'frente',\n",
       "  'enfermedad',\n",
       "  'anticuerpos',\n",
       "  'capaces',\n",
       "  'neutralizar',\n",
       "  'virus',\n",
       "  'farmacos',\n",
       "  'combatir',\n",
       "  'pandemia',\n",
       "  'covid19',\n",
       "  'decir',\n",
       "  'dinero',\n",
       "  'comprar',\n",
       "  'mascarillas',\n",
       "  'empleados',\n",
       "  'correosen',\n",
       "  'pagina',\n",
       "  'web',\n",
       "  'correos',\n",
       "  'explica',\n",
       "  'fecha',\n",
       "  '19',\n",
       "  'mayo',\n",
       "  'recaudado',\n",
       "  '25400',\n",
       "  '30000',\n",
       "  'necesitan',\n",
       "  'llevar',\n",
       "  'cabo',\n",
       "  'investigaciones',\n",
       "  'covid19',\n",
       "  'cuales',\n",
       "  'detallan',\n",
       "  'misma',\n",
       "  'paginacomo',\n",
       "  'puede',\n",
       "  'comprobar',\n",
       "  'ningun',\n",
       "  'lugar',\n",
       "  'indican',\n",
       "  'recaudado',\n",
       "  'vaya',\n",
       "  'destinarse',\n",
       "  'compra',\n",
       "  'material',\n",
       "  'proteccion',\n",
       "  'empleados',\n",
       "  'correos',\n",
       "  'tampoco',\n",
       "  'hace',\n",
       "  'referencia',\n",
       "  'ello',\n",
       "  'comunicado',\n",
       "  'publico',\n",
       "  'irsicaixa',\n",
       "  'anunciar',\n",
       "  'proyecto',\n",
       "  'tuit',\n",
       "  'agradecian',\n",
       "  'colaboracion',\n",
       "  'clientes',\n",
       "  'correosmuchas',\n",
       "  'gracias',\n",
       "  'clientes',\n",
       "  'correos',\n",
       "  'generosidad',\n",
       "  'permiten',\n",
       "  'seguir',\n",
       "  'avanzando',\n",
       "  'investigacion',\n",
       "  'sarscov2',\n",
       "  'httpstcowrtw6esaonpor',\n",
       "  'parte',\n",
       "  'pasado',\n",
       "  '7',\n",
       "  'mayo',\n",
       "  'publico',\n",
       "  'plataforma',\n",
       "  'contratacion',\n",
       "  'sector',\n",
       "  'publico',\n",
       "  'adjudicacion',\n",
       "  'contrato',\n",
       "  'compra',\n",
       "  '4750000',\n",
       "  'mascarillas',\n",
       "  'correos',\n",
       "  'valor',\n",
       "  '11815625',\n",
       "  'puedes',\n",
       "  'consultarlo',\n",
       "  'aqui']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 293)\n",
      "['11815625', '19', '2016', '2017', '25400', '30000', '4750000', '7', '700', 'acompanada', 'actual', 'adjudicacion', 'afirmando', 'agradecian', 'agroalimentarias', 'ahi', 'alarma', 'alicante', 'anchas', 'animal', 'animales', 'ano', 'anticuerpos', 'anunciar', 'aparecen', 'aqui', 'aquinos', 'aquise', 'articuloesta', 'asegura', 'atakoy', 'aunque', 'avanzando', 'ayudar', 'ballena', 'barcelona', 'brincos', 'brote', 'bulo', 'cabo', 'cada', 'cadena', 'califica', 'california', 'calle', 'campana', 'campando', 'canal', 'capaces', 'casas', 'center', 'centimo', 'centro', 'ciervo', 'ciervos', 'circula', 'circulado', 'circulando', 'cisnes', 'ciudad', 'ciudades', 'clientes', 'colaboracion', 'combatir', 'completo', 'compra', 'comprar', 'comprobar', 'comunicado', 'confinamiento', 'conjunto', 'consultarlo', 'contamos', 'contenido', 'contratacion', 'contrato', 'coronavirus', 'correos', 'correosen', 'correosmuchas', 'covid19', 'covid19desde', 'crisis', 'cuales', 'dando', 'debido', 'decir', 'decretado', 'dejado', 'delfines', 'denia', 'deportivo', 'desarrollar', 'destina', 'destinara', 'destinarse', 'detallan', 'dice', 'diciendo', 'dinero', 'donar', 'efectos', 'ejemplosesta', 'elementos', 'ello', 'embarcaciones', 'embargo', 'empleados', 'enfermedad', 'espacio', 'espana', 'estais', 'estambul', 'explica', 'explican', 'ezcaray', 'farmacos', 'fecha', 'figura', 'final', 'flamenco', 'fondos', 'fotos', 'frente', 'funcionarios', 'generosidad', 'gente', 'gobierno', 'grabadas', 'grabado', 'gracias', 'gran', 'guadalquivir', 'habeis', 'habria', 'hace', 'hacer', 'hacian', 'harbor', 'httpstcowrtw6esaonpor', 'huelva', 'humanos', 'imagen', 'imagenes', 'importe', 'inactividad', 'incorporado', 'indican', 'ingles', 'inmunicen', 'instituto', 'investigacion', 'investigaciones', 'irsicaixa', 'italia', 'italiana', 'johannesburgo', 'lanzado', 'leer', 'leerlo', 'leon', 'leone', 'libera', 'lillet', 'llegada', 'llevar', 'lucha', 'lugar', 'ma', 'mallorca', 'mantener', 'mar', 'marina', 'maritimo', 'mascarillas', 'matalascanas', 'material', 'mayo', 'medio', 'mensaje', 'misma', 'montaje', 'moraira', 'movido', 'mueve', 'municipio', 'nadando', 'necesitan', 'neutralizar', 'ningun', 'noche', 'objetivo', 'ocupado', 'ocurriendo', 'oficinas', 'pagar', 'pagina', 'paginacomo', 'pago', 'pagosegun', 'pallars', 'palma', 'pandemia', 'parecen', 'parte', 'pasado', 'pasarela', 'paseando', 'paseo', 'paso', 'permiten', 'plataforma', 'playa', 'pobla', 'podido', 'precios', 'preguntado', 'preguntando', 'premia', 'proteccion', 'provincia', 'provocada', 'proyecto', 'publicacion', 'publico', 'puede', 'pueden', 'puedes', 'puerto', 'real', 'realmente', 'recaudado', 'recaudar', 'red', 'redes', 'redondeando', 'redondear', 'redondeo', 'referencia', 'refleja', 'relacionandolo', 'repleta', 'restantes', 'rio', 'rosados', 'ruidera', 'rusia', 'ruso', 'sanidad', 'sanitarium', 'sarscov2', 'sector', 'seguir', 'semanas', 'servicios', 'sevilla', 'si', 'sido', 'sino', 'sociales', 'solidario', 'sudafricana', 'supercomputing', 'supuestamente', 'tambien', 'tampoco', 'tarjeta', 'tecnologia', 'teruel', 'tipo', 'tomada', 'tomadas', 'tomado', 'trata', 'tuit', 'turistas', 'turquia', 'utilicen', 'vacunas', 'valor', 'varias', 'vaya', 'ven', 'venecia', 'ventura', 'ver', 'vez', 'video', 'viraliza', 'viralizado', 'virus', 'web', 'whatsapp']\n",
      "[[0.         0.         0.04321662 0.04321662 0.         0.\n",
      "  0.         0.         0.04321662 0.04321662 0.12964987 0.\n",
      "  0.04321662 0.         0.         0.04321662 0.1728665  0.08643325\n",
      "  0.04321662 0.         0.04321662 0.04321662 0.         0.\n",
      "  0.04321662 0.03074898 0.08643325 0.04321662 0.08643325 0.04321662\n",
      "  0.04321662 0.08643325 0.         0.         0.04321662 0.06149797\n",
      "  0.04321662 0.04321662 0.15374492 0.         0.         0.\n",
      "  0.04321662 0.04321662 0.08643325 0.         0.04321662 0.08643325\n",
      "  0.         0.04321662 0.         0.         0.         0.04321662\n",
      "  0.08643325 0.03074898 0.04321662 0.12964987 0.08643325 0.1728665\n",
      "  0.04321662 0.         0.         0.         0.04321662 0.\n",
      "  0.         0.         0.         0.04321662 0.         0.\n",
      "  0.1728665  0.04321662 0.         0.         0.12299594 0.\n",
      "  0.         0.         0.06149797 0.         0.08643325 0.\n",
      "  0.04321662 0.04321662 0.         0.08643325 0.04321662 0.08643325\n",
      "  0.04321662 0.08643325 0.         0.         0.         0.\n",
      "  0.         0.03074898 0.04321662 0.         0.         0.04321662\n",
      "  0.04321662 0.         0.         0.04321662 0.12964987 0.\n",
      "  0.         0.04321662 0.12964987 0.04321662 0.04321662 0.\n",
      "  0.         0.04321662 0.         0.         0.         0.\n",
      "  0.08643325 0.         0.04321662 0.         0.         0.\n",
      "  0.04321662 0.04321662 0.04321662 0.1728665  0.         0.04321662\n",
      "  0.04321662 0.08643325 0.04321662 0.03074898 0.         0.04321662\n",
      "  0.04321662 0.         0.04321662 0.04321662 0.12964987 0.12964987\n",
      "  0.         0.04321662 0.         0.         0.04321662 0.\n",
      "  0.         0.         0.         0.         0.04321662 0.04321662\n",
      "  0.04321662 0.         0.08643325 0.04321662 0.04321662 0.04321662\n",
      "  0.04321662 0.04321662 0.04321662 0.         0.         0.\n",
      "  0.1728665  0.04321662 0.04321662 0.04321662 0.04321662 0.04321662\n",
      "  0.         0.04321662 0.         0.         0.03074898 0.04321662\n",
      "  0.         0.08643325 0.04321662 0.08643325 0.04321662 0.04321662\n",
      "  0.08643325 0.         0.         0.         0.04321662 0.\n",
      "  0.04321662 0.04321662 0.         0.         0.         0.\n",
      "  0.         0.         0.04321662 0.04321662 0.03074898 0.04321662\n",
      "  0.         0.         0.         0.04321662 0.04321662 0.04321662\n",
      "  0.         0.         0.08643325 0.04321662 0.04321662 0.\n",
      "  0.08643325 0.04321662 0.04321662 0.         0.04321662 0.04321662\n",
      "  0.         0.04321662 0.         0.         0.         0.09224695\n",
      "  0.21608312 0.12964987 0.04321662 0.         0.         0.\n",
      "  0.1728665  0.         0.         0.         0.         0.04321662\n",
      "  0.04321662 0.04321662 0.         0.04321662 0.08643325 0.04321662\n",
      "  0.04321662 0.04321662 0.         0.04321662 0.         0.\n",
      "  0.         0.04321662 0.         0.04321662 0.21608312 0.08643325\n",
      "  0.04321662 0.12964987 0.         0.04321662 0.         0.12964987\n",
      "  0.12964987 0.         0.         0.         0.04321662 0.04321662\n",
      "  0.04321662 0.04321662 0.04321662 0.06149797 0.         0.04321662\n",
      "  0.04321662 0.         0.         0.         0.04321662 0.\n",
      "  0.04321662 0.04321662 0.04321662 0.04321662 0.         0.345733\n",
      "  0.04321662 0.04321662 0.         0.         0.        ]\n",
      " [0.05724811 0.05724811 0.         0.         0.05724811 0.05724811\n",
      "  0.05724811 0.05724811 0.         0.         0.         0.05724811\n",
      "  0.         0.05724811 0.05724811 0.         0.         0.\n",
      "  0.         0.05724811 0.         0.         0.05724811 0.05724811\n",
      "  0.         0.04073251 0.         0.         0.         0.\n",
      "  0.         0.         0.05724811 0.05724811 0.         0.04073251\n",
      "  0.         0.         0.04073251 0.05724811 0.05724811 0.05724811\n",
      "  0.         0.         0.         0.05724811 0.         0.\n",
      "  0.05724811 0.         0.05724811 0.05724811 0.05724811 0.\n",
      "  0.         0.04073251 0.         0.         0.         0.\n",
      "  0.         0.17174434 0.05724811 0.05724811 0.         0.17174434\n",
      "  0.05724811 0.05724811 0.05724811 0.         0.05724811 0.05724811\n",
      "  0.         0.         0.05724811 0.05724811 0.04073251 0.51523303\n",
      "  0.05724811 0.05724811 0.12219752 0.05724811 0.         0.05724811\n",
      "  0.         0.         0.05724811 0.         0.         0.\n",
      "  0.         0.         0.05724811 0.05724811 0.05724811 0.05724811\n",
      "  0.05724811 0.04073251 0.         0.11449623 0.05724811 0.\n",
      "  0.         0.05724811 0.11449623 0.         0.         0.11449623\n",
      "  0.05724811 0.         0.         0.         0.         0.05724811\n",
      "  0.05724811 0.         0.05724811 0.05724811 0.05724811 0.05724811\n",
      "  0.         0.11449623 0.         0.11449623 0.05724811 0.05724811\n",
      "  0.         0.         0.         0.         0.05724811 0.\n",
      "  0.         0.         0.         0.04073251 0.05724811 0.\n",
      "  0.         0.05724811 0.         0.         0.         0.\n",
      "  0.05724811 0.         0.05724811 0.05724811 0.         0.05724811\n",
      "  0.05724811 0.22899246 0.05724811 0.11449623 0.         0.\n",
      "  0.         0.05724811 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05724811 0.05724811 0.05724811\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17174434 0.         0.05724811 0.11449623 0.04073251 0.\n",
      "  0.05724811 0.         0.         0.         0.         0.\n",
      "  0.         0.05724811 0.05724811 0.05724811 0.         0.11449623\n",
      "  0.         0.         0.05724811 0.05724811 0.05724811 0.05724811\n",
      "  0.05724811 0.05724811 0.         0.         0.04073251 0.\n",
      "  0.05724811 0.05724811 0.05724811 0.         0.         0.\n",
      "  0.05724811 0.05724811 0.         0.         0.         0.05724811\n",
      "  0.         0.         0.         0.11449623 0.         0.\n",
      "  0.11449623 0.         0.17174434 0.05724811 0.05724811 0.04073251\n",
      "  0.         0.         0.         0.17174434 0.11449623 0.05724811\n",
      "  0.         0.05724811 0.05724811 0.05724811 0.05724811 0.\n",
      "  0.         0.         0.05724811 0.         0.         0.\n",
      "  0.         0.         0.05724811 0.         0.05724811 0.05724811\n",
      "  0.05724811 0.         0.05724811 0.         0.         0.\n",
      "  0.         0.         0.05724811 0.         0.05724811 0.\n",
      "  0.         0.05724811 0.05724811 0.05724811 0.         0.\n",
      "  0.         0.         0.         0.04073251 0.05724811 0.\n",
      "  0.         0.05724811 0.05724811 0.05724811 0.         0.05724811\n",
      "  0.         0.         0.         0.         0.05724811 0.\n",
      "  0.         0.         0.05724811 0.05724811 0.05724811]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#TfidfVectorizer will L-2 normalize the output matrix by default, as a final step of the calculation. Having it normalized means it will have only weights between 0 and 1.\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, norm='l2')\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "#TfidfVectorizer ---> Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "#idfs are calculated by TfidfTransformer's fit()\n",
    "#tfidfs are calculated by TfidfTransformer's transform()\n",
    "#tfs are calculated by CountVectorizer's fit_transform()\n",
    "\n",
    "x_vovab = vectorizer.get_feature_names()\n",
    "x_mat = tfidf_matrix.todense()\n",
    "x_idf = vectorizer.idf_\n",
    "print(tfidf_matrix.shape)\n",
    "print(x_vovab)\n",
    "print(x_mat)\n",
    "\n",
    "#Posicion numero 36 -> 35 en la lista \"Barcelona\" aparece en ambos vectores de tf idf. La palabra aparece en los 2 documentos. En el primero 2 veces y en el 2º 1 vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
