{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open(\"Maldita_Dataset_27-06-2020\",'rb')\n",
    "dataset = pickle.load(infile)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para instalar el NLTK se me abre una iterfaz de usuario en otra ventana.\n",
    "\n",
    "#IMPORTS FROM BLOG\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions #--------------\n",
    "#import inflect #--------------\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer  #--------------\n",
    "\n",
    "#IMPORTS FROM NESTOR.\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.stem.snowball import SpanishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_keywords(words):\n",
    "    \"\"\"\n",
    "    Return the top10 frequent words.\n",
    "    :param list word: List containing all the words\n",
    "    :return: ordered list with the top-10 most frequent words (in descending order)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    counts = Counter(words)\n",
    "    ordered_list = sorted(words, key=counts.get, reverse=True)\n",
    "    unique_list = []\n",
    "    for ol in ordered_list:\n",
    "        if ol not in unique_list:\n",
    "            unique_list.append(ol)\n",
    "    return unique_list[:10]\n",
    "\n",
    "def show_top_10_keywords(processed_articles):\n",
    "    # Show the top-10 most frequent words\n",
    "    for article_words in processed_articles:\n",
    "        # Get te top-10 words\n",
    "        top_10_words = get_top_10_keywords(article_words)\n",
    "        #print(article_words)\n",
    "        print(\"\\nTOP 10 words:\")\n",
    "        for w in top_10_words:\n",
    "            print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circula por WhatsApp una cadena en la que se dice que Correos estaÃÅ \"redondeando\" los precios de sus servicios y que ese dinero se destina para \"pagar mascarillas y elementos de proteccioÃÅn para los funcionarios de Correos\". Es un bulo: se trata de una campanÃÉa que han lanzado con el objetivo de recaudar fondos para la investigacioÃÅn contra la COVID-19.Desde Correos nos explican que han \"incorporado en la red de oficinas la figura del 'redondeo solidario' en su pasarela de pago, para ayudar a recaudar fondos y hacer frente al COVID‚Äê19\". Para ello, \"los clientes de Correos pueden redondear el importe final de su compra y donar los ceÃÅntimos restantes a la lucha contra el coronavirus cada vez que utilicen su tarjeta como medio de pago\".SeguÃÅn Correos, todo lo recaudado se destinaraÃÅ a \"un proyecto conjunto de IrsiCaixa con el Barcelona Supercomputing Center y el Centro de InvestigacioÃÅn en Sanidad Animal del Instituto de InvestigacioÃÅn y TecnologiÃÅa Agroalimentarias\" que tiene como objetivo \"desarrollar vacunas que inmunicen frente a la enfermedad, anticuerpos capaces de neutralizar el virus y faÃÅrmacos para combatir la pandemia del COVID-19\". Es decir, que el dinero no es para comprar mascarillas para los empleados de Correos.En su paÃÅgina web, Correos explica que, a fecha de 19 de mayo, se han recaudado 25.400‚Ç¨ de los 30.000‚Ç¨ que necesitan para llevar a cabo las investigaciones contra la COVID-19, las cuales detallan en esa misma paÃÅgina:Como se puede comprobar, en ninguÃÅn lugar indican que lo recaudado vaya a destinarse a la compra de material de proteccioÃÅn para los empleados de Correos. Tampoco se hace referencia a ello en el comunicado que publicoÃÅ IrsiCaixa para anunciar el proyecto, ni en el tuit en el que agradeciÃÅan la colaboracioÃÅn de los clientes de Correos.Muchas gracias a los clientes de @Correos, que con su generosidad nos permiten seguir avanzando en la investigacioÃÅn contra el SARS-CoV-2 üí™ https://t.co/WRtW6esaOnPor otra parte, el pasado 7 de mayo se publicoÃÅ en la Plataforma de ContratacioÃÅn del Sector PuÃÅblico la adjudicacioÃÅn del contrato de compra de 4.750.000 mascarillas para Correos, por un valor de 11.815.625‚Ç¨. Puedes consultarlo aquiÃÅ.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0:5][1][\"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#De los diccionarios de articulos ** me quedo solo con el contenido de x articulos del dataset y los introduzco en una lista\n",
    "#de contenidos.\n",
    "# **article = {\"Title\": title, \"Publication_date\": date, \"Content\": content, \"URL\": url, \"Fuente\": \"Maldita.es\"}\n",
    "\n",
    "content_list = []\n",
    "for a in dataset[0:2]:\n",
    "    content_list.append(a['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 1...\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      "en\n",
      ",\n",
      "el\n",
      ".\n",
      "la\n",
      "un\n",
      "que\n",
      "por\n",
      "se\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      "que\n",
      "la\n",
      ",\n",
      "el\n",
      "en\n",
      "para\n",
      "a\n",
      "Correos\n",
      "los\n"
     ]
    }
   ],
   "source": [
    "def exercise_1(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 1:\n",
    "        * Extract the top-10 most frequent words appearing in the articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 1...\")\n",
    "    # Show the top-10 most frequent words\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        # Get the list of words in the article\n",
    "        article_words = word_tokenize(article)\n",
    "        processed_articles.append(article_words)\n",
    "    return processed_articles\n",
    "\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_1(content_list)\n",
    "#print(processed_articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 2...\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      ".\n",
      "(\n",
      ")\n",
      "viÃÅdeo\n",
      "puerto\n",
      "bulo\n",
      "si\n",
      "alarma\n",
      "coronavirus\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      "Correos\n",
      "``\n",
      "''\n",
      ".\n",
      "mascarillas\n",
      ":\n",
      "clientes\n",
      "compra\n",
      "recaudado\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_all_news(news, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Method to remove the stop words from a sentence.\n",
    "    :param str sentence: List of all news\n",
    "    :param list stopwords: List containing the stopwords to remove from the sentences\n",
    "    :return: list of news with the stopwords removed\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    important_words_in_news=[]\n",
    "    for one_news in news:\n",
    "        new_words = []\n",
    "        # Split the sentence into separate words\n",
    "        words = word_tokenize(one_news)\n",
    "        #print(words)\n",
    "        \n",
    "        for word in words:\n",
    "        # Only keep the important words\n",
    "            if word not in custom_stopwords:\n",
    "                new_words.append(word)         \n",
    "        important_words_in_news.append(new_words)\n",
    "        \n",
    "    return important_words_in_news\n",
    "\n",
    "\n",
    "\n",
    "def exercise_2(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 2: Most of the results are stopwords. Let's remove them from the articles first\n",
    "        * Perform some normalization:\n",
    "            * Remove the stopwords (we can also use custom stop words)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 2...\")\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_2(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "* Convierte el texto a min√∫scula\n",
    "* Quita las stopwords\n",
    "* Quita signos de puntuaci√≥n\n",
    "* Quita los acentos\n",
    "* Extrae las 10 palabras m√°s frecuentes de los art√≠culos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 3...\n",
      "<class 'list'>\n",
      "['durante el estado de alarma por el coronavirus han circulado fotos y viÃÅdeos de animales campando a sus anchas por varias ciudades. ciervos, delfines o flamencos que, supuestamente, han ocupado el espacio que los humanos les hemos dejado durante el confinamiento. sin embargo, aunque hay imaÃÅgenes de este tipo se han podido ver estas semanas, algunas por las que nos habeÃÅis preguntado se han tomado antes de la crisis del coronavirus o son un montaje. os contamos algunos ejemplos.estaÃÅ circulando un viÃÅdeo en redes sociales donde aparecen unos delfines nadando en un puerto deportivo, supuestamente debido a la inactividad del puerto provocada por la crisis sanitaria del coronavirus. este viÃÅdeo se ha movido diciendo que se trata del paseo mariÃÅtimo de palma de mallorca, el puerto de denia (alicante), el de moraira (alicante) o el de premiaÃÄ de mar (barcelona), pero es un bulo. estas imaÃÅgenes fueron grabadas en el puerto deportivo de atakoÃày en la ciudad de estambul (turquiÃÅa). te lo contamos aquiÃÅ.nos estaÃÅis preguntando mucho si un viÃÅdeo en el que se ven ciervos paseando por una calle, que circula afirmando que es en el municipio de ruidera, provincia de ciudad real, estaÃÅ realmente grabado ahiÃÅ y durante el estado de alarma. tambieÃÅn se ha movido como si fuera teruel, la pobla de lillet (barcelona), ezcaray o el pallars, pero es un bulo: este viÃÅdeo no es actual y no estaÃÅ grabado en espanÃÉa, sino en italia. puedes leer maÃÅs aquiÃÅ.nos habeÃÅis preguntado tambieÃÅn por un viÃÅdeo de un ciervo dando brincos en la playa que se mueve como si hubiera sido grabado en la playa de matalascanÃÉas (huelva) durante el estado de alarma decretado en espanÃÉa por el covid-19. sin embargo, el viÃÅdeo no es actual. puedes leer maÃÅs en este artiÃÅculo.estaÃÅ circulando en redes sociales una publicacioÃÅn acompanÃÉada de una imagen del gran canal de la ciudad italiana de venecia repleta de lo que califica de \\'cisnes rosados\\', aunque parecen flamencos, como si fuera real. el contenido asegura que refleja \"la llegada de los cisnes rosados, que no lo haciÃÅan hace muchos anÃÉos, ya que por efectos de la pandemia, no hay embarcaciones con turistas\" en el canal. sin embargo, es un bulo: se trata de un montaje. te contamos maÃÅs aquiÃÅ.se ha viralizado en redes un viÃÅdeo de una ballena nadando en un puerto que supuestamente habriÃÅa sido grabado en el riÃÅo guadalquivir a su paso por sevilla y relacionaÃÅndolo con el estado de alarma decretado en espanÃÉa por la covid-19. es un bulo: las imaÃÅgenes fueron tomadas en 2017 en el ventura harbor marina (california). te contamos maÃÅs en este artiÃÅculo.estaÃÅ circulando por redes sociales la imagen de un leoÃÅn en medio de la calle durante la noche como si estuviera ocurriendo en rusia por el actual brote de coronavirus. el mensaje que se viraliza (tambieÃÅn en ingleÃÅs) dice: \"gobierno ruso libera a 700 leones para mantener a la gente en sus casas\". es un bulo. la imagen es real, pero fue tomada en la ciudad sudafricana de johannesburgo en 2016. puedes leerlo completo aquiÃÅ.', 'circula por whatsapp una cadena en la que se dice que correos estaÃÅ \"redondeando\" los precios de sus servicios y que ese dinero se destina para \"pagar mascarillas y elementos de proteccioÃÅn para los funcionarios de correos\". es un bulo: se trata de una campanÃÉa que han lanzado con el objetivo de recaudar fondos para la investigacioÃÅn contra la covid-19.desde correos nos explican que han \"incorporado en la red de oficinas la figura del \\'redondeo solidario\\' en su pasarela de pago, para ayudar a recaudar fondos y hacer frente al covid‚Äê19\". para ello, \"los clientes de correos pueden redondear el importe final de su compra y donar los ceÃÅntimos restantes a la lucha contra el coronavirus cada vez que utilicen su tarjeta como medio de pago\".seguÃÅn correos, todo lo recaudado se destinaraÃÅ a \"un proyecto conjunto de irsicaixa con el barcelona supercomputing center y el centro de investigacioÃÅn en sanidad animal del instituto de investigacioÃÅn y tecnologiÃÅa agroalimentarias\" que tiene como objetivo \"desarrollar vacunas que inmunicen frente a la enfermedad, anticuerpos capaces de neutralizar el virus y faÃÅrmacos para combatir la pandemia del covid-19\". es decir, que el dinero no es para comprar mascarillas para los empleados de correos.en su paÃÅgina web, correos explica que, a fecha de 19 de mayo, se han recaudado 25.400‚Ç¨ de los 30.000‚Ç¨ que necesitan para llevar a cabo las investigaciones contra la covid-19, las cuales detallan en esa misma paÃÅgina:como se puede comprobar, en ninguÃÅn lugar indican que lo recaudado vaya a destinarse a la compra de material de proteccioÃÅn para los empleados de correos. tampoco se hace referencia a ello en el comunicado que publicoÃÅ irsicaixa para anunciar el proyecto, ni en el tuit en el que agradeciÃÅan la colaboracioÃÅn de los clientes de correos.muchas gracias a los clientes de @correos, que con su generosidad nos permiten seguir avanzando en la investigacioÃÅn contra el sars-cov-2 üí™ https://t.co/wrtw6esaonpor otra parte, el pasado 7 de mayo se publicoÃÅ en la plataforma de contratacioÃÅn del sector puÃÅblico la adjudicacioÃÅn del contrato de compra de 4.750.000 mascarillas para correos, por un valor de 11.815.625‚Ç¨. puedes consultarlo aquiÃÅ.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'punctuation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-ed0536e1217d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Obtengo las keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mprocessed_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexercise_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;31m# Muestro las top-10 keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mshow_top_10_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-203-ed0536e1217d>\u001b[0m in \u001b[0;36mexercise_3\u001b[1;34m(articles)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Add all the punctuation signs to the list of stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mcustom_stopwords\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# lowercase all the stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'punctuation'"
     ]
    }
   ],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def exercise_3(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 3: There is further preprocessing we can perform\n",
    "        * Perform some normalization:\n",
    "            * Lowercase all words\n",
    "            * Remove all the stopwords\n",
    "            * Remove punctuation signs\n",
    "            * Remove all accents from the words\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 3...\")\n",
    "    \n",
    "    #print(articles)\n",
    "    \n",
    "    # Lowercase all the articles\n",
    "    lower_articles = to_lowercase(articles)\n",
    "    #print(lower_articles)\n",
    "    \n",
    "    # Create a translation table\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'√°√©√≠√≥√∫√º'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    \n",
    "    print(type(string))\n",
    "    accent_string = [article.translate(table) for article in lower_articles]\n",
    "    print(accent_string)\n",
    "    \n",
    "    # Prepare our custom list of stopwords\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    \n",
    "    # Add all the punctuation signs to the list of stopwords\n",
    "    custom_stopwords += list(string.punctuation)\n",
    "    \n",
    "    # lowercase all the stopwords\n",
    "    my_stopwords = to_lowercase(custom_stopwords)\n",
    "    \n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(accent_articles, my_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_3(content_list)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['durante el estado de alarma por el coronavirus han circulado fotos y viÃÅdeos de animales campando', 'Hola hola hoae']\n"
     ]
    }
   ],
   "source": [
    "#No puedo quitar las tildes porque tienen un formato \"extra√±o\"\n",
    "table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'√°√©√≠√≥√∫√º'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "\n",
    "string = ['durante el estado de alarma por el coronavirus han circulado fotos y viÃÅdeos de animales campando', 'H√≥la h√≥la ho√°√©']\n",
    "print(type(string))\n",
    "accent_string = [article.translate(table) for article in string]\n",
    "print(accent_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
