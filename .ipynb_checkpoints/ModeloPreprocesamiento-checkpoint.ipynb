{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.stem.snowball import SpanishStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news():\n",
    "    \"\"\"\n",
    "    Read a TXT file and extract the full text from the news.\n",
    "    :return: list of news\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    # Notimex\n",
    "    input_file = 'news_notimex_exercises.txt'\n",
    "    f = codecs.open(input_file, encoding='utf-8')\n",
    "    lines_notimex = f.readlines()\n",
    "    f.close()\n",
    "    # Agencia EFE\n",
    "    input_file = 'news_agencia_efe_exercises.txt'\n",
    "    f = codecs.open(input_file, encoding='utf-8')\n",
    "    lines_agencia_efe = f.readlines()\n",
    "    f.close()\n",
    "    lines_news = lines_notimex + lines_agencia_efe\n",
    "    return lines_news\n",
    "\n",
    "\n",
    "def get_top_10_keywords(words):\n",
    "    \"\"\"\n",
    "    Return the top10 frequent words.\n",
    "    :param list word: List containing all the words\n",
    "    :return: ordered list with the top-10 most frequent words (in descending order)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    counts = Counter(words)\n",
    "    ordered_list = sorted(words, key=counts.get, reverse=True)\n",
    "    unique_list = []\n",
    "    for ol in ordered_list:\n",
    "        if ol not in unique_list:\n",
    "            unique_list.append(ol)\n",
    "    return unique_list[:10]\n",
    "\n",
    "def show_top_10_keywords(processed_articles):\n",
    "    # Show the top-10 most frequent words\n",
    "    for article_words in processed_articles:\n",
    "        # Get te top-10 words\n",
    "        top_10_words = get_top_10_keywords(article_words)\n",
    "        #print(article_words)\n",
    "        print(\"\\nTOP 10 words:\")\n",
    "        for w in top_10_words:\n",
    "            print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the list with the news\n",
    "all_news = read_news()\n",
    "# Get 5 news articles\n",
    "articles = all_news[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "Extrae las 10 palabras más frecuentes del artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 1...\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      "que\n",
      "en\n",
      "la\n",
      ".\n",
      "por\n",
      "el\n",
      ",\n",
      "y\n",
      "(\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      "de\n",
      "la\n",
      "y\n",
      "en\n",
      "del\n",
      "el\n",
      "que\n",
      ".\n",
      "los\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      ",\n",
      "la\n",
      "“\n",
      "”\n",
      "y\n",
      "a\n",
      "por\n",
      ".\n",
      "México\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      ",\n",
      "que\n",
      "y\n",
      "en\n",
      "el\n",
      ".\n",
      "del\n",
      "la\n",
      "para\n",
      "\n",
      "TOP 10 words:\n",
      "de\n",
      ",\n",
      "la\n",
      "el\n",
      "y\n",
      "del\n",
      "los\n",
      "en\n",
      "que\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "def exercise_1(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 1:\n",
    "        * Extract the top-10 most frequent words appearing in the articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 1...\")\n",
    "    # Show the top-10 most frequent words\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        # Get the list of words in the article\n",
    "        article_words = word_tokenize(article)\n",
    "        processed_articles.append(article_words)\n",
    "    return processed_articles\n",
    "\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_1(articles)\n",
    "#print(processed_articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "* Quita las stopwords\n",
    "* Extrae las 10 palabras más frecuentes de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 2...\n",
      "\n",
      "TOP 10 words:\n",
      ".\n",
      ",\n",
      "(\n",
      ")\n",
      "La\n",
      "solo\n",
      "amonestados\n",
      "jugadores\n",
      "final\n",
      "título\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      ".\n",
      "“\n",
      "”\n",
      "comercio\n",
      "(\n",
      ")\n",
      "Alianza\n",
      "Mercosur\n",
      "Pacífico\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      "“\n",
      "”\n",
      ".\n",
      "México\n",
      "Radio\n",
      "UNAM\n",
      "Universidad\n",
      "Nacional\n",
      "Autónoma\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      ".\n",
      "México\n",
      "”\n",
      "TLCAN\n",
      "“\n",
      "Carstens\n",
      "integración\n",
      "beneficios\n",
      "económica\n",
      "\n",
      "TOP 10 words:\n",
      ",\n",
      ":\n",
      ".\n",
      "Tabasco\n",
      "SITEM\n",
      "2017\n",
      "secretario\n",
      "Educación\n",
      "En\n",
      "Diego\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_all_news(news, custom_stopwords):\n",
    "    \"\"\"\n",
    "    Method to remove the stop words from a sentence.\n",
    "    :param str sentence: List of all news\n",
    "    :param list stopwords: List containing the stopwords to remove from the sentences\n",
    "    :return: list of news with the stopwords removed\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    important_words_in_news=[]\n",
    "    for one_news in news:\n",
    "        # Split the sentence into separate words\n",
    "        # TODO\n",
    "        # Only keep the important words\n",
    "        # TODO\n",
    "    return important_words_in_news\n",
    "\n",
    "\n",
    "\n",
    "def exercise_2(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 2: Most of the results are stopwords. Let's remove them from the articles first\n",
    "        * Perform some normalization:\n",
    "            * Remove the stopwords (we can also use custom stop words)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 2...\")\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_2(articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "* Convierte el texto a minúscula\n",
    "* Quita las stopwords\n",
    "* Quita signos de puntuación\n",
    "* Quita los acentos\n",
    "* Extrae las 10 palabras más frecuentes de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 3...\n",
      "\n",
      "TOP 10 words:\n",
      "solo\n",
      "amonestados\n",
      "jugadores\n",
      "final\n",
      "titulo\n",
      "mexico\n",
      "7\n",
      "abr\n",
      "notimex\n",
      ".-\n",
      "\n",
      "TOP 10 words:\n",
      "comercio\n",
      "“\n",
      "”\n",
      "alianza\n",
      "mercosur\n",
      "ministros\n",
      "pacifico\n",
      "bloques\n",
      "encuentro\n",
      "argentina\n",
      "\n",
      "TOP 10 words:\n",
      "“\n",
      "”\n",
      "mexico\n",
      "radio\n",
      "unam\n",
      "universidad\n",
      "nacional\n",
      "autonoma\n",
      "violencia\n",
      "equidad\n",
      "\n",
      "TOP 10 words:\n",
      "mexico\n",
      "mas\n",
      "”\n",
      "tlcan\n",
      "“\n",
      "carstens\n",
      "integracion\n",
      "beneficios\n",
      "economica\n",
      "unidos\n",
      "\n",
      "TOP 10 words:\n",
      "educacion\n",
      "sitem\n",
      "tabasco\n",
      "2017\n",
      "secretario\n",
      "gobierno\n",
      "estatal\n",
      "diego\n",
      "animas\n",
      "dijo\n"
     ]
    }
   ],
   "source": [
    "def exercise_3(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 3: There is further preprocessing we can perform\n",
    "        * Perform some normalization:\n",
    "            * Lowercase all words\n",
    "            * Remove all the stopwords\n",
    "            * Remove punctuation signs\n",
    "            * Remove all accents from the words\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 3...\")\n",
    "    # Lowercase all the articles\n",
    "    # TODO\n",
    "    \n",
    "    # Create a translation table\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    # Use the translation table to remove all the accents\n",
    "    articles = [article.translate(table) for article in articles]\n",
    "    # Prepare our custom list of stopwords\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    # Add all the punctuation signs to the list of stopwords\n",
    "    custom_stopwords += list(string.punctuation)\n",
    "    # lowercase all the stopwords\n",
    "    # TODO\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    return articles_no_stopwords\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_3(articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4\n",
    "* Convierte el texto a minúscula\n",
    "* Quita las stopwords\n",
    "* Quita signos de puntuación\n",
    "* Quita los acentos\n",
    "* Obtén el \"stem\" de las palabras\n",
    "* Extrae las 10 palabras más frecuentes de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE 4...\n",
      "\n",
      "TOP 10 words:\n",
      "sol\n",
      "amonest\n",
      "jugador\n",
      "final\n",
      "titul\n",
      "mexic\n",
      "7\n",
      "abr\n",
      "notimex\n",
      ".-\n",
      "\n",
      "TOP 10 words:\n",
      "comerci\n",
      "“\n",
      "”\n",
      "ministr\n",
      "alianz\n",
      "mercosur\n",
      "pacif\n",
      "bloqu\n",
      "encuentr\n",
      "argentin\n",
      "\n",
      "TOP 10 words:\n",
      "“\n",
      "”\n",
      "mexic\n",
      "radi\n",
      "unam\n",
      "univers\n",
      "nacional\n",
      "autonom\n",
      "expresion\n",
      "violenci\n",
      "\n",
      "TOP 10 words:\n",
      "mexic\n",
      "benefici\n",
      "mas\n",
      "”\n",
      "tlcan\n",
      "econom\n",
      "“\n",
      "carstens\n",
      "integracion\n",
      "pued\n",
      "\n",
      "TOP 10 words:\n",
      "educacion\n",
      "sitem\n",
      "tabasc\n",
      "trabaj\n",
      "respons\n",
      "sal\n",
      "buen\n",
      "2017\n",
      "secretari\n",
      "gobiern\n"
     ]
    }
   ],
   "source": [
    "def exercise_4(articles):\n",
    "    \"\"\"\n",
    "    EXERCISE 4: There is further preprocessing we can perform\n",
    "        * Perform some normalization:\n",
    "            * Lowercase all words\n",
    "            * Remove all the stopwords\n",
    "            * Remove punctuation signs\n",
    "            * Remove all accents from the words\n",
    "            * Stem all the words (Stemming = obtaining something similar to the \"root\" form of the word)\n",
    "        * Extract the top-10 most frequent words appearing in the normalized articles\n",
    "    :param list articles: List containing all the news we will process\n",
    "    :return: list of news tokenized into words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    print(\"EXERCISE 4...\")\n",
    "    # Lowercase all the articles\n",
    "    articles = [article.lower() for article in articles]\n",
    "    # Create a translation table\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    # Use the translation table to remove all the accents\n",
    "    articles = [article.translate(table) for article in articles]\n",
    "    # Prepare our custom list of stopwords\n",
    "    custom_stopwords = [\"Noticia\", \"Noticias\"]\n",
    "    custom_stopwords += stopwords.words('spanish')\n",
    "    # Add all the punctuation signs to the list of stopwords\n",
    "    custom_stopwords += list(string.punctuation)\n",
    "    # lowercase all the stopwords\n",
    "    # TODO\n",
    "    articles_no_stopwords = remove_stopwords_from_all_news(articles, custom_stopwords)\n",
    "    # Get the spanish stemmer\n",
    "    stemmer = SpanishStemmer()\n",
    "    stemmed_articles = []\n",
    "    for article in articles_no_stopwords:\n",
    "        article_words = []\n",
    "        # The article contains all the words. Separate them\n",
    "        for word in article:\n",
    "            # TODO\n",
    "        # The next step expects a string. Let's concatenate the separate words using blanks.\n",
    "        stemmed_articles.append(article_words)\n",
    "    return stemmed_articles\n",
    "\n",
    "\n",
    "# Obtengo las keywords\n",
    "processed_articles = exercise_4(articles)\n",
    "# Muestro las top-10 keywords\n",
    "show_top_10_keywords(processed_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
